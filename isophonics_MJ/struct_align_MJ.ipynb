{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select audio segments for music similarity comparison\n",
    "\n",
    "Matching Segment Selection (my own method based on hierarchical structure analysis)\n",
    "Padding to longest duration in dataset\n",
    "Padding between pair of pieces being compared\n",
    "Truncating to shortest duration in dataset\n",
    "Truncating between pair of pieces being compared\n",
    "Fixed-length 15s, 30s, 60s segment from the middle\n",
    "Audio Thumbnail of 15s, 30s, 60s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## > Library importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computation\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.interpolate import interp2d\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "#Data Processing\n",
    "import sklearn.cluster\n",
    "import sklearn\n",
    "\n",
    "#Audio\n",
    "import librosa\n",
    "from librosa import display\n",
    "from pyAudioAnalysis.audioSegmentation import music_thumbnailing\n",
    "\n",
    "#System\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#Pickling\n",
    "import dill\n",
    "\n",
    "#Reading\n",
    "import reader\n",
    "import segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## > Load annotations and audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose parent directory containing audiofiles and annotations\n",
    "directory = '/Users/chris/Google Drive/Publication Files/CMMR2021/Datasets/isophonics_MJ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotation paths\n",
    "lab_paths = reader.read_paths(directory, '.lab')\n",
    "# Load to dictionary and fix annotation data\n",
    "labs = reader.load_lab(lab_paths)\n",
    "print(\"Loaded annotations.\")\n",
    "\n",
    "# Cross reference audio with annotations\n",
    "ref = reader.ref_paths(directory, directory)\n",
    "\n",
    "# Load audio paths\n",
    "audio_paths = reader.read_paths(directory, '.flac')\n",
    "file_no = len(audio_paths)\n",
    "# Load audio\n",
    "audio = {}\n",
    "sr = 22050\n",
    "for i,path in enumerate(audio_paths):\n",
    "    audio[os.path.basename(path)[:-5]] = librosa.load(path, sr=sr, mono=True)\n",
    "    sys.stdout.write(\"\\rLoaded %i/%i pieces.\" % (i+1, file_no))\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## > Get segments from each method"
   ]
  },
  {
   "source": [
    "### >> Matching Segment Selection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "\n",
    "#### >>> Compute segmentation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmin = 2\n",
    "kmax = 7\n",
    "mss_seg_ids = {}\n",
    "mss_f = {} #formatted segments\n",
    "for p in audio_paths:\n",
    "    name = os.path.basename(p[:-5])\n",
    "    y, sr = audio[name]\n",
    "    mss_seg_ids[name], mss_f[name] = segment.segment(y, sr, kmin, kmax)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the segments\n",
    "n_to_plot = 2\n",
    "fig, axs = plt.subplots(n_to_plot, 1, figsize=(20, 4*n_to_plot))\n",
    "for i,p in enumerate(audio_paths):\n",
    "    name = os.path.basename(p[:-5])\n",
    "    axs[i].matshow(mss_seg_ids[name], aspect=10)\n",
    "    axs[i].set(title=name)\n",
    "    if i>=n_to_plot-1:\n",
    "        break\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### >>> Find segment hits, keep unique hit that is closest-positioned for each segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "level1 = {} \n",
    "\n",
    "#traverse primary audio file\n",
    "for p1 in audio_paths: \n",
    "    name1 = os.path.basename(p1)[:-5]\n",
    "\n",
    "    #dictionary storing hits dictionary of secondary audio files\n",
    "    level2 = {}\n",
    "\n",
    "    #traverse secondary audio file\n",
    "    for p2 in audio_paths: \n",
    "        name2 = os.path.basename(p2)[:-5]\n",
    "        if name1 == name2: #don't compare a song with itself\n",
    "            continue\n",
    "\n",
    "        #traverse hierarchies name1\n",
    "        for i in range(kmax-kmin):\n",
    "            #traverse segments name1\n",
    "            for j in range(len(mss_f[name1][2][i])): #2: formatted beats at #i hierarchy\n",
    "\n",
    "                #dictionary storing the unique hit for each segment that is closest positioned \n",
    "                #to it in the format (hierarchy_level, position_index)\n",
    "                level3 = {}\n",
    "                #traverse hierarchies\n",
    "                for k in range(kmax-kmin):\n",
    "                    #traverse segments\n",
    "                    for l in range(len(mss_f[name][2][k])):\n",
    "\n",
    "                        #if segments have the same number of beats (#1: number of beats)\n",
    "                        if mss_f[name1][i][j][1] == mss_f[name2][k][l][1]:\n",
    "\n",
    "                            #if in hit dictionary already\n",
    "                            if (i,j) in level3:\n",
    "                                #get value\n",
    "                                m,n = level3[(i,j)]\n",
    "                                #check if it's closer than existing entry, replace\n",
    "                                if abs(mss_f[name1][i][j][0]-mss_f[name2][k][l][0]) < abs(mss_f[name1][i][j][0]-mss_f[name2][m][n][0]):\n",
    "                                    level3[(i,j)] = (k,l)\n",
    "                            #if not in hit dictionary already\n",
    "                                level3[(i,j)] = (k,l)\n",
    "        level2[name2] = level3\n",
    "    level1[name1] = level2\n",
    "\n"
   ]
  },
  {
   "source": [
    "#### >>> Only keep the largest segments"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### >> Pad to maximum"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### >>> Pad annotation data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get maximum length of any audiofile in frames\n",
    "max_length = 0\n",
    "for p in audio_paths:\n",
    "    name = os.path.basename(p[:-5])\n",
    "    if len(audio[name][0]) > max_length:\n",
    "        max_length = len(audio[name][0])\n",
    "\n",
    "labs_PM = {}\n",
    "for p in audio_paths:\n",
    "    name = os.path.basename(p[:-5])\n",
    "    labs_PM[name] = reader.vectorize(lab=labs[name], sr=sr, start_f=0, end_f=max_length)"
   ]
  },
  {
   "source": [
    "#### >>> PM scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### >> Pad pairwise"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### >>> Compute pairwise padding of annotation data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "essentially create directed distance matrix by returning \n",
    "segments of A in comparison with B when querrying labs_PP[A][B]\n",
    "and segments of B in comparison with A when querrying labs_PP[B][A]\n",
    "\"\"\"\n",
    "labs_PP = {}\n",
    "for p1 in audio_paths:\n",
    "    name1 = os.path.basename(p1)[:-5]\n",
    "    d = {} #2D dictionary\n",
    "    for p2 in audio_paths\n",
    "        name2 = os.path.basename(p2)[:-5]\n",
    "        #find length of longer audiofile\n",
    "        max_length = max(len(audio[name1][0]), len(audio[name2][0]))\n",
    "        d[name2] = reader.vectorize(lab=labs[name1], sr=sr, start_f=0, end_f=max_length)\n",
    "    labs_PP[name1] = d\n"
   ]
  },
  {
   "source": [
    "#### >>> PP scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### >> Truncate to minimum\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### >>> Truncate annotation data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get minimum length of any audiofile\n",
    "min_length = len((audio[os.path.basename(audio_paths[0])[:-5]])[0])\n",
    "for p in audio_paths:\n",
    "    name = os.path.basename(p[:-5])\n",
    "    if len(audio[name][0]) < min_length:\n",
    "        min_length = len(audio[name][0])\n",
    "\n",
    "labs_TM = {}\n",
    "for p in audio_paths:\n",
    "    name = os.path.basename(p[:-5])\n",
    "    labs_TM[name] = reader.vectorize(lab=labs[name], sr=sr, start_f=0, end_f=min_length)"
   ]
  },
  {
   "source": [
    "#### >>> TM scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### >> Truncate pairwise"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### >>> Compute pairwise truncation of annotation data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "essentially create directed distance matrix by returning \n",
    "segments of A in comparison with B when querrying labs_PP[A][B]\n",
    "and segments of B in comparison with A when querrying labs_PP[B][A]\n",
    "\"\"\"\n",
    "labs_TP = {}\n",
    "for p1 in audio_paths:\n",
    "    name1 = os.path.basename(p1)[:-5]\n",
    "    d = {} #2D dictionary\n",
    "    for p2 in audio_paths\n",
    "        name2 = os.path.basename(p2)[:-5]\n",
    "        #find length of longer audiofile\n",
    "        max_length = min(len(audio[name1][0]), len(audio[name2][0]))\n",
    "        d[name2] = reader.vectorize(lab=labs[name1], sr=sr, start_f=0, end_f=min_length)\n",
    "    labs_TP[name1] = d"
   ]
  },
  {
   "source": [
    "#### >>> TP scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### >> Fixed length from middle"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### >> Get annotations for given interval around the middle of the audio"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_FL15 = {}\n",
    "labs_FL30 = {}\n",
    "labs_FL60 = {}\n",
    "\n",
    "for p in audio_paths:\n",
    "\n",
    "    name = os.path.basename(p[:-5])\n",
    "    audio_length = len(audio[name][0]) #get audio length\n",
    "\n",
    "    start_f = int(audio_length/2) - int(7.5*sr) #get start of segment, centered around the middle\n",
    "    end_f = start_f + 15*sr #get end of segment, centered around the middle\n",
    "    labs_FL15[name] = reader.vectorize(lab=labs[name], sr=sr, start_f=start_f, end_f=end_f)\n",
    "\n",
    "    start_f = int(audio_length/2)- 15*sr #get start of segment, centered around the middle\n",
    "    end_f = start_f + 30*sr #get end of segment, centered around the middle\n",
    "    labs_FL30[name] = reader.vectorize(lab=labs[name], sr=sr, start_f=start_f, end_f=end_f)\n",
    "\n",
    "    start_f = int(audio_length/2) - 30*sr #get start of segment, centered around the middle\n",
    "    end_f = start_f + 60*sr #get end of segment, centered around the middle\n",
    "    labs_FL60[name] = reader.vectorize(lab=labs[name], sr=sr, start_f=start_f, end_f=end_f)"
   ]
  },
  {
   "source": [
    "#### >>> FL scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### >> Audio thumbnailing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### >>> Compute thumbnails timestamps on annotation data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labs_AT15 = {}\n",
    "labs_AT30 = {}\n",
    "labs_AT60 = {}\n",
    "\n",
    "for p in audio_paths:\n",
    "\n",
    "    name = os.path.basename(p[:-5])\n",
    "    audio_length = len(audio[name][0]) #get audio length\n",
    "\n",
    "    start_t1, end_t1, start_t2, end_t2 = music_thumbnailing(y, sr, thumb_size=15.) \n",
    "    start_f = time_to_frames(start_t1, sr=sr, hop_length=1)\n",
    "    end_f = time_to_frames(end_t1, sr=sr, hop_length=1)\n",
    "    labs_AT15[name] = reader.vectorize(lab=labs[name], sr=sr, start_f=start_f, end_f=end_f)\n",
    "\n",
    "    start_t1, end_t1, start_t2, end_t2 = music_thumbnailing(y, sr, thumb_size=30.) \n",
    "    start_f = time_to_frames(start_t1, sr=sr, hop_length=1)\n",
    "    end_f = time_to_frames(end_t1, sr=sr, hop_length=1)\n",
    "    labs_AT30[name] = reader.vectorize(lab=labs[name], sr=sr, start_f=start_f, end_f=end_f)\n",
    "\n",
    "    start_t1, end_t1, start_t2, end_t2 = music_thumbnailing(y, sr, thumb_size=60.) \n",
    "    start_f = time_to_frames(start_t1, sr=sr, hop_length=1)\n",
    "    end_f = time_to_frames(end_t1, sr=sr, hop_length=1)\n",
    "    labs_AT60[name] = reader.vectorize(lab=labs[name], sr=sr, start_f=start_f, end_f=end_f)"
   ]
  },
  {
   "source": [
    "#### >>> AT scores"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python394jvsc74a57bd03043fb70fd2df7e9357aa7649f2e828b48d620d4fe7e46ef23f0096f1ce7edaf",
   "display_name": "Python 3.9.4 64-bit ('.venv': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "3043fb70fd2df7e9357aa7649f2e828b48d620d4fe7e46ef23f0096f1ce7edaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}